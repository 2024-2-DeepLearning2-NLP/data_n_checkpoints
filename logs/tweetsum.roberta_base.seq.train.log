Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x79094d51eee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 4092 of the training set: {'input_ids': [0, 44799, 16, 13689, 59, 5, 544, 147, 51, 32, 3276, 7, 1514, 4591, 8, 67, 4496, 14, 42, 16, 45, 447, 4, 50118, 45443, 27620, 13, 5, 1254, 8, 553, 7, 18695, 455, 766, 8, 1286, 33000, 4, 2, 2, 0, 100, 23126, 216, 596, 381, 6526, 2812, 20524, 16, 5, 129, 248, 18941, 2214, 38, 64, 465, 15, 787, 1225, 4432, 4652, 2, 0, 1039, 37401, 20695, 11468, 96, 25685, 328, 2615, 47, 1137, 201, 99, 247, 110, 1316, 16, 278, 7, 116, 166, 581, 1649, 383, 66, 1589, 3764, 2, 0, 1039, 32110, 4591, 347, 5347, 315, 532, 328, 2, 0, 1039, 37401, 20695, 274, 35997, 7344, 52, 581, 28, 441, 7, 33, 70, 9, 49, 3686, 1010, 6, 53, 89, 18, 8574, 59, 13703, 1383, 259, 35, 1205, 640, 90, 4, 876, 73, 288, 118, 398, 534, 642, 757, 257, 32259, 1589, 3764, 2, 0, 1039, 32110, 4591, 347, 5347, 20227, 1942, 594, 328, 2, 0, 1039, 37401, 20695, 12487, 13095, 328, 370, 216, 147, 7, 465, 201, 114, 89, 18, 932, 1493, 47, 240, 244, 19, 4, 1205, 640, 90, 4, 876, 73, 257, 717, 330, 7083, 282, 46073, 30083, 17841, 23171, 1589, 3764, 2, 0, 1039, 32110, 4591, 347, 5347, 3837, 47, 328, 17841, 14285, 2, 0, 1039, 37401, 20695, 370, 214, 460, 2814, 328, 17841, 23171, 1589, 3764, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2, 2, 0, 2, 0, 1, 0, 0]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 6594
Loss at step 10: 1.0836
Loss at step 20: 1.0351
Loss at step 30: 1.0434
Loss at step 40: 1.1058
Loss at step 50: 1.1191
Loss at step 60: 1.0741
Loss at step 70: 1.1018
Loss at step 80: 1.1172
Loss at step 90: 1.0986
Loss at step 100: 1.1006
Loss at step 110: 1.1470
Loss at step 120: 1.1196
Loss at step 130: 1.1133
Loss at step 140: 1.1357
Loss at step 150: 1.0644
Loss at step 160: 1.0965
Loss at step 170: 1.0954
Loss at step 180: 1.0850
Loss at step 190: 1.0936
Loss at step 200: 1.1228
Loss at step 210: 1.0687
Loss at step 220: 1.0868
Loss at step 230: 1.1023
Loss at step 240: 1.1183
Loss at step 250: 1.0414
Loss at step 260: 1.0735
Loss at step 270: 1.0968
Loss at step 280: 1.1182
Loss at step 290: 1.0736
Loss at step 300: 1.0729
Loss at step 310: 1.1516
Loss at step 320: 1.0836
Loss at step 330: 1.0956
Loss at step 340: 1.1318
Loss at step 350: 1.0658
Loss at step 360: 1.0825
Loss at step 370: 1.0577
Loss at step 380: 1.0866
Loss at step 390: 1.0535
Loss at step 400: 1.0456
Loss at step 410: 1.0732
Loss at step 420: 1.1040
Loss at step 430: 1.0593
Loss at step 440: 1.0692
Loss at step 450: 1.0958
Loss at step 460: 1.0384
Loss at step 470: 1.1536
Loss at step 480: 1.1237
Loss at step 490: 1.0723
Loss at step 500: 1.0905
Loss at step 510: 1.0526
Loss at step 520: 1.0844
Loss at step 530: 1.0985
Loss at step 540: 1.1370
Loss at step 550: 1.1036
Loss at step 560: 1.1183
Loss at step 570: 1.0826
Loss at step 580: 1.0973
Loss at step 590: 1.0726
Loss at step 600: 1.0901
Loss at step 610: 1.0303
Loss at step 620: 1.0696
Loss at step 630: 1.0799
Loss at step 640: 1.1041
Loss at step 650: 1.0792
Loss at step 660: 1.1043
Loss at step 670: 1.1169
Loss at step 680: 1.0592
Loss at step 690: 1.0580
Loss at step 700: 1.0931
Loss at step 710: 1.1097
Loss at step 720: 1.0835
Loss at step 730: 1.0747
Loss at step 740: 1.1139
Loss at step 750: 1.0752
Loss at step 760: 1.0929
Loss at step 770: 1.0501
Loss at step 780: 1.1003
Loss at step 790: 1.1072
Loss at step 800: 1.1330
Loss at step 810: 1.1273
Loss at step 820: 1.0982
Loss at step 830: 1.0988
Loss at step 840: 1.0879
Loss at step 850: 1.0962
Loss at step 860: 1.0683
Loss at step 870: 1.1317
Loss at step 880: 1.0909
Loss at step 890: 1.0752
Loss at step 900: 1.0613
Loss at step 910: 1.0892
Loss at step 920: 1.0741
Loss at step 930: 1.0833
Loss at step 940: 1.0590
Loss at step 950: 1.0590
Loss at step 960: 1.0762
Loss at step 970: 1.0791
Loss at step 980: 1.1017
Loss at step 990: 1.1016
Loss at step 1000: 1.0729
Loss at step 1010: 1.0946
Loss at step 1020: 1.1049
Loss at step 1030: 1.0925
Loss at step 1040: 1.0843
Loss at step 1050: 1.1123
Loss at step 1060: 1.0754
Loss at step 1070: 1.0972
Loss at step 1080: 1.0916
Loss at step 1090: 1.1001
Loss at step 1100: 1.1054
Loss at step 1110: 1.0776
Loss at step 1120: 1.0888
Loss at step 1130: 1.0659
Loss at step 1140: 1.0939
Loss at step 1150: 1.1140
Loss at step 1160: 1.1044
Loss at step 1170: 1.1112
Loss at step 1180: 1.0689
Loss at step 1190: 1.0931
Loss at step 1200: 1.0970
Loss at step 1210: 1.0651
Loss at step 1220: 1.1085
Loss at step 1230: 1.1184
Loss at step 1240: 1.0876
Loss at step 1250: 1.0666
Loss at step 1260: 1.1024
Loss at step 1270: 1.1511
Loss at step 1280: 1.0638
Loss at step 1290: 1.1009
Loss at step 1300: 1.0810
Loss at step 1310: 1.1085
Loss at step 1320: 1.1049
Loss at step 1330: 1.0900
Loss at step 1340: 1.0654
Loss at step 1350: 1.0982
Loss at step 1360: 1.1011
Loss at step 1370: 1.0691
Loss at step 1380: 1.0724
Loss at step 1390: 1.1219
Loss at step 1400: 1.1043
Loss at step 1410: 1.0945
Loss at step 1420: 1.1053
Loss at step 1430: 1.0722
Loss at step 1440: 1.0814
Loss at step 1450: 1.1153
Loss at step 1460: 1.0975
Loss at step 1470: 1.1208
Loss at step 1480: 1.0822
Loss at step 1490: 1.0698
Loss at step 1500: 1.0969
Loss at step 1510: 1.0519
Loss at step 1520: 1.1150
Loss at step 1530: 1.0631
Loss at step 1540: 1.0960
Loss at step 1550: 1.1062
Loss at step 1560: 1.0898
Loss at step 1570: 1.0944
Loss at step 1580: 1.0912
Loss at step 1590: 1.0954
Loss at step 1600: 1.1146
Loss at step 1610: 1.1002
Loss at step 1620: 1.1276
Loss at step 1630: 1.0700
Loss at step 1640: 1.0940
Loss at step 1650: 1.0951
Loss at step 1660: 1.0994
Loss at step 1670: 1.0677
Loss at step 1680: 1.1089
Loss at step 1690: 1.0942
Loss at step 1700: 1.0849
Loss at step 1710: 1.0809
Loss at step 1720: 1.1109
Loss at step 1730: 1.0824
Loss at step 1740: 1.0923
Loss at step 1750: 1.0860
Loss at step 1760: 1.1128
Loss at step 1770: 1.0783
Loss at step 1780: 1.0917
Loss at step 1790: 1.0735
Loss at step 1800: 1.1231
Loss at step 1810: 1.1143
Loss at step 1820: 1.0438
Loss at step 1830: 1.0678
Loss at step 1840: 1.0977
Loss at step 1850: 1.0953
Loss at step 1860: 1.0706
Loss at step 1870: 1.1225
Loss at step 1880: 1.0737
Loss at step 1890: 1.0642
Loss at step 1900: 1.1075
Loss at step 1910: 1.0875
Loss at step 1920: 1.0750
Loss at step 1930: 1.1031
Loss at step 1940: 1.0704
Loss at step 1950: 1.0875
Loss at step 1960: 1.0748
Loss at step 1970: 1.0955
Loss at step 1980: 1.0782
Loss at step 1990: 1.1135
Loss at step 2000: 1.0680
Loss at step 2010: 1.0735
Loss at step 2020: 1.0821
Loss at step 2030: 1.0683
Loss at step 2040: 1.0728
Loss at step 2050: 1.0711
Loss at step 2060: 1.0896
Loss at step 2070: 1.0782
Loss at step 2080: 1.0557
Loss at step 2090: 1.0897
Loss at step 2100: 1.0977
Loss at step 2110: 1.0283
Loss at step 2120: 1.1073
Loss at step 2130: 1.0970
Loss at step 2140: 1.0472
Loss at step 2150: 1.0953
Loss at step 2160: 1.0653
Loss at step 2170: 1.1308
Loss at step 2180: 1.0592
Loss at step 2190: 1.1103
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.370304, 'precision': [0.0, 0.370304, 0.0], 'recall': [0.0, 1.0, 0.0], 'f1': [0.0, 0.540469, 0.0]}
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x75aa27b09d30> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 4092 of the training set: {'input_ids': [0, 44799, 16, 13689, 59, 5, 544, 147, 51, 32, 3276, 7, 1514, 4591, 8, 67, 4496, 14, 42, 16, 45, 447, 4, 50118, 45443, 27620, 13, 5, 1254, 8, 553, 7, 18695, 455, 766, 8, 1286, 33000, 4, 2, 2, 0, 100, 23126, 216, 596, 381, 6526, 2812, 20524, 16, 5, 129, 248, 18941, 2214, 38, 64, 465, 15, 787, 1225, 4432, 4652, 2, 0, 1039, 37401, 20695, 11468, 96, 25685, 328, 2615, 47, 1137, 201, 99, 247, 110, 1316, 16, 278, 7, 116, 166, 581, 1649, 383, 66, 1589, 3764, 2, 0, 1039, 32110, 4591, 347, 5347, 315, 532, 328, 2, 0, 1039, 37401, 20695, 274, 35997, 7344, 52, 581, 28, 441, 7, 33, 70, 9, 49, 3686, 1010, 6, 53, 89, 18, 8574, 59, 13703, 1383, 259, 35, 1205, 640, 90, 4, 876, 73, 288, 118, 398, 534, 642, 757, 257, 32259, 1589, 3764, 2, 0, 1039, 32110, 4591, 347, 5347, 20227, 1942, 594, 328, 2, 0, 1039, 37401, 20695, 12487, 13095, 328, 370, 216, 147, 7, 465, 201, 114, 89, 18, 932, 1493, 47, 240, 244, 19, 4, 1205, 640, 90, 4, 876, 73, 257, 717, 330, 7083, 282, 46073, 30083, 17841, 23171, 1589, 3764, 2, 0, 1039, 32110, 4591, 347, 5347, 3837, 47, 328, 17841, 14285, 2, 0, 1039, 37401, 20695, 370, 214, 460, 2814, 328, 17841, 23171, 1589, 3764, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2, 2, 0, 2, 0, 1, 0, 0]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 6594
Loss at step 10: 1.0836
Loss at step 20: 1.0351
Loss at step 30: 1.0434
Loss at step 40: 1.1058
Loss at step 50: 1.1191
Loss at step 60: 1.0741
Loss at step 70: 1.1018
Loss at step 80: 1.1172
Loss at step 90: 1.0986
Loss at step 100: 1.1006
Loss at step 110: 1.1470
Loss at step 120: 1.1196
Loss at step 130: 1.1133
Loss at step 140: 1.1357
Loss at step 150: 1.0644
Loss at step 160: 1.0965
Loss at step 170: 1.0954
Loss at step 180: 1.0850
Loss at step 190: 1.0936
Loss at step 200: 1.1228
Loss at step 210: 1.0687
Loss at step 220: 1.0868
Loss at step 230: 1.1023
Loss at step 240: 1.1183
Loss at step 250: 1.0414
Loss at step 260: 1.0735
Loss at step 270: 1.0968
Loss at step 280: 1.1182
Loss at step 290: 1.0736
Loss at step 300: 1.0729
Loss at step 310: 1.1516
Loss at step 320: 1.0836
Loss at step 330: 1.0956
Loss at step 340: 1.1318
Loss at step 350: 1.0658
Loss at step 360: 1.0825
Loss at step 370: 1.0577
Loss at step 380: 1.0866
Loss at step 390: 1.0535
Loss at step 400: 1.0456
Loss at step 410: 1.0732
Loss at step 420: 1.1040
Loss at step 430: 1.0593
Loss at step 440: 1.0692
Loss at step 450: 1.0958
Loss at step 460: 1.0384
Loss at step 470: 1.1536
Loss at step 480: 1.1237
Loss at step 490: 1.0723
Loss at step 500: 1.0905
Loss at step 510: 1.0526
Loss at step 520: 1.0844
Loss at step 530: 1.0985
Loss at step 540: 1.1370
Loss at step 550: 1.1036
Loss at step 560: 1.1183
Loss at step 570: 1.0826
Loss at step 580: 1.0973
Loss at step 590: 1.0726
Loss at step 600: 1.0901
Loss at step 610: 1.0303
Loss at step 620: 1.0696
Loss at step 630: 1.0799
Loss at step 640: 1.1041
Loss at step 650: 1.0792
Loss at step 660: 1.1043
Loss at step 670: 1.1169
Loss at step 680: 1.0592
Loss at step 690: 1.0580
Loss at step 700: 1.0931
Loss at step 710: 1.1097
Loss at step 720: 1.0835
Loss at step 730: 1.0747
Loss at step 740: 1.1139
Loss at step 750: 1.0752
Loss at step 760: 1.0929
Loss at step 770: 1.0501
Loss at step 780: 1.1003
Loss at step 790: 1.1072
Loss at step 800: 1.1330
Loss at step 810: 1.1273
Loss at step 820: 1.0982
Loss at step 830: 1.0988
Loss at step 840: 1.0879
Loss at step 850: 1.0962
Loss at step 860: 1.0683
Loss at step 870: 1.1317
Loss at step 880: 1.0909
Loss at step 890: 1.0752
Loss at step 900: 1.0613
Loss at step 910: 1.0892
Loss at step 920: 1.0741
Loss at step 930: 1.0833
Loss at step 940: 1.0590
Loss at step 950: 1.0590
Loss at step 960: 1.0762
Loss at step 970: 1.0791
Loss at step 980: 1.1017
Loss at step 990: 1.1016
Loss at step 1000: 1.0729
Loss at step 1010: 1.0946
Loss at step 1020: 1.1049
Loss at step 1030: 1.0925
Loss at step 1040: 1.0843
Loss at step 1050: 1.1123
Loss at step 1060: 1.0754
Loss at step 1070: 1.0972
Loss at step 1080: 1.0916
Loss at step 1090: 1.1001
Loss at step 1100: 1.1054
Loss at step 1110: 1.0776
Loss at step 1120: 1.0888
Loss at step 1130: 1.0659
Loss at step 1140: 1.0939
Loss at step 1150: 1.1140
Loss at step 1160: 1.1044
Loss at step 1170: 1.1112
Loss at step 1180: 1.0689
Loss at step 1190: 1.0931
Loss at step 1200: 1.0970
Loss at step 1210: 1.0651
Loss at step 1220: 1.1085
Loss at step 1230: 1.1184
Loss at step 1240: 1.0876
Loss at step 1250: 1.0666
Loss at step 1260: 1.1024
Loss at step 1270: 1.1511
Loss at step 1280: 1.0638
Loss at step 1290: 1.1009
Loss at step 1300: 1.0810
Loss at step 1310: 1.1085
Loss at step 1320: 1.1049
Loss at step 1330: 1.0900
Loss at step 1340: 1.0654
Loss at step 1350: 1.0982
Loss at step 1360: 1.1011
Loss at step 1370: 1.0691
Loss at step 1380: 1.0724
Loss at step 1390: 1.1219
Loss at step 1400: 1.1043
Loss at step 1410: 1.0945
Loss at step 1420: 1.1053
Loss at step 1430: 1.0722
Loss at step 1440: 1.0814
Loss at step 1450: 1.1153
Loss at step 1460: 1.0975
Loss at step 1470: 1.1208
Loss at step 1480: 1.0822
Loss at step 1490: 1.0698
Loss at step 1500: 1.0969
Loss at step 1510: 1.0519
Loss at step 1520: 1.1150
Loss at step 1530: 1.0631
Loss at step 1540: 1.0960
Loss at step 1550: 1.1062
Loss at step 1560: 1.0898
Loss at step 1570: 1.0944
Loss at step 1580: 1.0912
Loss at step 1590: 1.0954
Loss at step 1600: 1.1146
Loss at step 1610: 1.1002
Loss at step 1620: 1.1276
Loss at step 1630: 1.0700
Loss at step 1640: 1.0940
Loss at step 1650: 1.0951
Loss at step 1660: 1.0994
Loss at step 1670: 1.0677
Loss at step 1680: 1.1089
Loss at step 1690: 1.0942
Loss at step 1700: 1.0849
Loss at step 1710: 1.0809
Loss at step 1720: 1.1109
Loss at step 1730: 1.0824
Loss at step 1740: 1.0923
Loss at step 1750: 1.0860
Loss at step 1760: 1.1128
Loss at step 1770: 1.0783
Loss at step 1780: 1.0917
Loss at step 1790: 1.0735
Loss at step 1800: 1.1231
Loss at step 1810: 1.1143
Loss at step 1820: 1.0438
Loss at step 1830: 1.0678
Loss at step 1840: 1.0977
Loss at step 1850: 1.0953
Loss at step 1860: 1.0706
Loss at step 1870: 1.1225
Loss at step 1880: 1.0737
Loss at step 1890: 1.0642
Loss at step 1900: 1.1075
Loss at step 1910: 1.0875
Loss at step 1920: 1.0750
Loss at step 1930: 1.1031
Loss at step 1940: 1.0704
Loss at step 1950: 1.0875
Loss at step 1960: 1.0748
Loss at step 1970: 1.0955
Loss at step 1980: 1.0782
Loss at step 1990: 1.1135
Loss at step 2000: 1.0680
Loss at step 2010: 1.0735
Loss at step 2020: 1.0821
Loss at step 2030: 1.0683
Loss at step 2040: 1.0728
Loss at step 2050: 1.0711
Loss at step 2060: 1.0896
Loss at step 2070: 1.0782
Loss at step 2080: 1.0557
Loss at step 2090: 1.0897
Loss at step 2100: 1.0977
Loss at step 2110: 1.0283
Loss at step 2120: 1.1073
Loss at step 2130: 1.0970
Loss at step 2140: 1.0472
Loss at step 2150: 1.0953
Loss at step 2160: 1.0653
Loss at step 2170: 1.1308
Loss at step 2180: 1.0592
Loss at step 2190: 1.1103
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.370304, 'precision': [0.0, 0.370304, 0.0], 'recall': [0.0, 1.0, 0.0], 'f1': [0.0, 0.540469, 0.0]}
