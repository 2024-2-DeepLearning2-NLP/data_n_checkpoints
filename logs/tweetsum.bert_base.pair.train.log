Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x739371cb28b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 83320 of the training set: {'input_ids': [101, 1030, 2310, 21885, 5644, 6279, 6442, 9951, 2326, 2525, 25141, 3064, 1996, 2799, 2099, 1998, 2117, 2051, 1996, 4274, 3632, 2041, 102, 8013, 2003, 17949, 2055, 1996, 4274, 2326, 2029, 2003, 2025, 2551, 1012, 4005, 2163, 2008, 2023, 3277, 2003, 2949, 1998, 11186, 2005, 1996, 3861, 1997, 1996, 4070, 1998, 2036, 2163, 2008, 2027, 2031, 1037, 3553, 2298, 2046, 2023, 3277, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 8655
Loss at step 10: 0.9841
Loss at step 20: 1.0290
Loss at step 30: 0.8958
Loss at step 40: 1.1063
Loss at step 50: 0.7665
Loss at step 60: 1.1717
Loss at step 70: 0.7787
Loss at step 80: 0.8785
Loss at step 90: 0.9307
Loss at step 100: 0.9444
Loss at step 110: 0.8200
Loss at step 120: 0.9162
Loss at step 130: 1.0273
Loss at step 140: 0.9849
Loss at step 150: 0.8466
Loss at step 160: 0.9728
Loss at step 170: 0.9802
Loss at step 180: 0.9027
Loss at step 190: 0.8232
Loss at step 200: 1.0056
Loss at step 210: 0.7596
Loss at step 220: 0.9751
Loss at step 230: 0.8198
Loss at step 240: 0.9254
Loss at step 250: 0.9247
Loss at step 260: 0.9077
Loss at step 270: 0.8100
Loss at step 280: 0.8513
Loss at step 290: 0.9319
Loss at step 300: 0.7880
Loss at step 310: 0.7348
Loss at step 320: 0.7221
Loss at step 330: 0.9254
Loss at step 340: 0.6225
Loss at step 350: 0.7834
Loss at step 360: 0.7118
Loss at step 370: 0.6483
Loss at step 380: 0.8001
Loss at step 390: 0.8113
Loss at step 400: 1.0467
Loss at step 410: 0.6732
Loss at step 420: 0.7971
Loss at step 430: 0.6171
Loss at step 440: 0.7005
Loss at step 450: 0.6471
Loss at step 460: 0.7324
Loss at step 470: 0.7721
Loss at step 480: 0.7328
Loss at step 490: 0.7172
Loss at step 500: 0.7507
Loss at step 510: 0.4797
Loss at step 520: 0.5074
Loss at step 530: 0.8072
Loss at step 540: 0.5815
Loss at step 550: 0.6806
Loss at step 560: 0.4999
Loss at step 570: 0.6190
Loss at step 580: 0.8453
Loss at step 590: 0.7843
Loss at step 600: 0.6234
Loss at step 610: 0.6992
Loss at step 620: 0.6448
Loss at step 630: 0.5057
Loss at step 640: 0.7299
Loss at step 650: 0.7094
Loss at step 660: 0.5062
Loss at step 670: 0.6774
Loss at step 680: 0.6283
Loss at step 690: 0.6846
Loss at step 700: 0.5493
Loss at step 710: 0.5691
Loss at step 720: 0.9779
Loss at step 730: 0.6309
Loss at step 740: 0.4806
Loss at step 750: 0.5491
Loss at step 760: 0.5017
Loss at step 770: 0.5622
Loss at step 780: 0.4795
Loss at step 790: 0.7014
Loss at step 800: 0.4830
Loss at step 810: 0.5776
Loss at step 820: 0.6509
Loss at step 830: 0.5568
Loss at step 840: 0.4010
Loss at step 850: 0.4974
Loss at step 860: 0.5387
Loss at step 870: 0.5582
Loss at step 880: 0.5453
Loss at step 890: 0.6176
Loss at step 900: 0.5763
Loss at step 910: 0.4497
Loss at step 920: 0.3930
Loss at step 930: 0.7440
Loss at step 940: 0.5648
Loss at step 950: 0.3751
Loss at step 960: 0.4530
Loss at step 970: 0.3602
Loss at step 980: 0.4863
Loss at step 990: 0.4477
Loss at step 1000: 0.6402
Loss at step 1010: 0.4505
Loss at step 1020: 0.6069
Loss at step 1030: 0.3506
Loss at step 1040: 0.3192
Loss at step 1050: 0.3252
Loss at step 1060: 0.4955
Loss at step 1070: 0.2392
Loss at step 1080: 0.6029
Loss at step 1090: 0.3922
Loss at step 1100: 0.4412
Loss at step 1110: 0.2713
Loss at step 1120: 0.5940
Loss at step 1130: 0.3809
Loss at step 1140: 0.2848
Loss at step 1150: 0.3968
Loss at step 1160: 0.4733
Loss at step 1170: 0.4110
Loss at step 1180: 0.3930
Loss at step 1190: 0.3940
Loss at step 1200: 0.3136
Loss at step 1210: 0.3323
Loss at step 1220: 0.4394
Loss at step 1230: 0.5120
Loss at step 1240: 0.5331
Loss at step 1250: 0.2209
Loss at step 1260: 0.5174
Loss at step 1270: 0.3539
Loss at step 1280: 0.3272
Loss at step 1290: 0.2755
Loss at step 1300: 0.2823
Loss at step 1310: 0.3093
Loss at step 1320: 0.5923
Loss at step 1330: 0.2910
Loss at step 1340: 0.3607
Loss at step 1350: 0.3962
Loss at step 1360: 0.2842
Loss at step 1370: 0.2095
Loss at step 1380: 0.4533
Loss at step 1390: 0.6763
Loss at step 1400: 0.5128
Loss at step 1410: 0.3425
Loss at step 1420: 0.2530
Loss at step 1430: 0.2287
Loss at step 1440: 0.5867
Loss at step 1450: 0.4016
Loss at step 1460: 0.3813
Loss at step 1470: 0.2885
Loss at step 1480: 0.3369
Loss at step 1490: 0.2600
Loss at step 1500: 0.1841
Loss at step 1510: 0.2779
Loss at step 1520: 0.3581
Loss at step 1530: 0.2951
Loss at step 1540: 0.2065
Loss at step 1550: 0.1987
Loss at step 1560: 0.3485
Loss at step 1570: 0.2698
Loss at step 1580: 0.3865
Loss at step 1590: 0.3896
Loss at step 1600: 0.2466
Loss at step 1610: 0.3812
Loss at step 1620: 0.3473
Loss at step 1630: 0.4037
Loss at step 1640: 0.1446
Loss at step 1650: 0.1553
Loss at step 1660: 0.3325
Loss at step 1670: 0.2351
Loss at step 1680: 0.2893
Loss at step 1690: 0.3029
Loss at step 1700: 0.3171
Loss at step 1710: 0.2751
Loss at step 1720: 0.6470
Loss at step 1730: 0.1657
Loss at step 1740: 0.3221
Loss at step 1750: 0.2411
Loss at step 1760: 0.0865
Loss at step 1770: 0.4187
Loss at step 1780: 0.2692
Loss at step 1790: 0.1600
Loss at step 1800: 0.2243
Loss at step 1810: 0.0409
Loss at step 1820: 0.3599
Loss at step 1830: 0.0302
Loss at step 1840: 0.3276
Loss at step 1850: 0.0949
Loss at step 1860: 0.2553
Loss at step 1870: 0.2002
Loss at step 1880: 0.3865
Loss at step 1890: 0.2003
Loss at step 1900: 0.3068
Loss at step 1910: 0.4065
Loss at step 1920: 0.2804
Loss at step 1930: 0.1097
Loss at step 1940: 0.1051
Loss at step 1950: 0.3668
Loss at step 1960: 0.1804
Loss at step 1970: 0.1477
Loss at step 1980: 0.1498
Loss at step 1990: 0.1201
Loss at step 2000: 0.1025
Loss at step 2010: 0.1473
Loss at step 2020: 0.1483
Loss at step 2030: 0.1949
Loss at step 2040: 0.2670
Loss at step 2050: 0.2770
Loss at step 2060: 0.2588
Loss at step 2070: 0.2377
Loss at step 2080: 0.1684
Loss at step 2090: 0.3641
Loss at step 2100: 0.2151
Loss at step 2110: 0.1357
Loss at step 2120: 0.2886
Loss at step 2130: 0.1116
Loss at step 2140: 0.1089
Loss at step 2150: 0.1784
Loss at step 2160: 0.1312
Loss at step 2170: 0.2139
Loss at step 2180: 0.2347
Loss at step 2190: 0.3288
Loss at step 2200: 0.2386
Loss at step 2210: 0.0676
Loss at step 2220: 0.1972
Loss at step 2230: 0.1036
Loss at step 2240: 0.0774
Loss at step 2250: 0.2666
Loss at step 2260: 0.0287
Loss at step 2270: 0.1570
Loss at step 2280: 0.1273
Loss at step 2290: 0.2660
Loss at step 2300: 0.0484
Loss at step 2310: 0.1516
Loss at step 2320: 0.2596
Loss at step 2330: 0.1695
Loss at step 2340: 0.1364
Loss at step 2350: 0.1104
Loss at step 2360: 0.3662
Loss at step 2370: 0.3430
Loss at step 2380: 0.1043
Loss at step 2390: 0.2644
Loss at step 2400: 0.1831
Loss at step 2410: 0.1205
Loss at step 2420: 0.1908
Loss at step 2430: 0.0245
Loss at step 2440: 0.2222
Loss at step 2450: 0.0678
Loss at step 2460: 0.0560
Loss at step 2470: 0.1911
Loss at step 2480: 0.1823
Loss at step 2490: 0.0390
Loss at step 2500: 0.2693
Loss at step 2510: 0.0558
Loss at step 2520: 0.0517
Loss at step 2530: 0.2532
Loss at step 2540: 0.0922
Loss at step 2550: 0.2112
Loss at step 2560: 0.1012
Loss at step 2570: 0.2007
Loss at step 2580: 0.1866
Loss at step 2590: 0.0946
Loss at step 2600: 0.0605
Loss at step 2610: 0.0855
Loss at step 2620: 0.3647
Loss at step 2630: 0.1106
Loss at step 2640: 0.0887
Loss at step 2650: 0.0314
Loss at step 2660: 0.2518
Loss at step 2670: 0.1581
Loss at step 2680: 0.1755
Loss at step 2690: 0.0903
Loss at step 2700: 0.4588
Loss at step 2710: 0.2394
Loss at step 2720: 0.1574
Loss at step 2730: 0.1427
Loss at step 2740: 0.4324
Loss at step 2750: 0.0714
Loss at step 2760: 0.0957
Loss at step 2770: 0.2676
Loss at step 2780: 0.2173
Loss at step 2790: 0.0314
Loss at step 2800: 0.1898
Loss at step 2810: 0.4292
Loss at step 2820: 0.0549
Loss at step 2830: 0.0997
Loss at step 2840: 0.0445
Loss at step 2850: 0.2681
Loss at step 2860: 0.0734
Loss at step 2870: 0.1469
Loss at step 2880: 0.0212
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.547215, 'precision': [0.679834, 0.507346, 0.467002], 'recall': [0.524038, 0.653382, 0.428489], 'f1': [0.591855, 0.571177, 0.446917]}
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x7983989e9700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 83320 of the training set: {'input_ids': [101, 1030, 2310, 21885, 5644, 6279, 6442, 9951, 2326, 2525, 25141, 3064, 1996, 2799, 2099, 1998, 2117, 2051, 1996, 4274, 3632, 2041, 102, 8013, 2003, 17949, 2055, 1996, 4274, 2326, 2029, 2003, 2025, 2551, 1012, 4005, 2163, 2008, 2023, 3277, 2003, 2949, 1998, 11186, 2005, 1996, 3861, 1997, 1996, 4070, 1998, 2036, 2163, 2008, 2027, 2031, 1037, 3553, 2298, 2046, 2023, 3277, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 8655
Loss at step 10: 0.9841
Loss at step 20: 1.0290
Loss at step 30: 0.8958
Loss at step 40: 1.1063
Loss at step 50: 0.7665
Loss at step 60: 1.1717
Loss at step 70: 0.7787
Loss at step 80: 0.8785
Loss at step 90: 0.9307
Loss at step 100: 0.9444
Loss at step 110: 0.8200
Loss at step 120: 0.9162
Loss at step 130: 1.0273
Loss at step 140: 0.9849
Loss at step 150: 0.8466
Loss at step 160: 0.9728
Loss at step 170: 0.9802
Loss at step 180: 0.9027
Loss at step 190: 0.8232
Loss at step 200: 1.0056
Loss at step 210: 0.7596
Loss at step 220: 0.9751
Loss at step 230: 0.8198
Loss at step 240: 0.9254
Loss at step 250: 0.9247
Loss at step 260: 0.9077
Loss at step 270: 0.8100
Loss at step 280: 0.8513
Loss at step 290: 0.9319
Loss at step 300: 0.7880
Loss at step 310: 0.7348
Loss at step 320: 0.7221
Loss at step 330: 0.9254
Loss at step 340: 0.6225
Loss at step 350: 0.7834
Loss at step 360: 0.7118
Loss at step 370: 0.6483
Loss at step 380: 0.8001
Loss at step 390: 0.8113
Loss at step 400: 1.0467
Loss at step 410: 0.6732
Loss at step 420: 0.7971
Loss at step 430: 0.6171
Loss at step 440: 0.7005
Loss at step 450: 0.6471
Loss at step 460: 0.7324
Loss at step 470: 0.7721
Loss at step 480: 0.7328
Loss at step 490: 0.7172
Loss at step 500: 0.7507
Loss at step 510: 0.4797
Loss at step 520: 0.5074
Loss at step 530: 0.8072
Loss at step 540: 0.5815
Loss at step 550: 0.6806
Loss at step 560: 0.4999
Loss at step 570: 0.6190
Loss at step 580: 0.8453
Loss at step 590: 0.7843
Loss at step 600: 0.6234
Loss at step 610: 0.6992
Loss at step 620: 0.6448
Loss at step 630: 0.5057
Loss at step 640: 0.7299
Loss at step 650: 0.7094
Loss at step 660: 0.5062
Loss at step 670: 0.6774
Loss at step 680: 0.6283
Loss at step 690: 0.6846
Loss at step 700: 0.5493
Loss at step 710: 0.5691
Loss at step 720: 0.9779
Loss at step 730: 0.6309
Loss at step 740: 0.4806
Loss at step 750: 0.5491
Loss at step 760: 0.5017
Loss at step 770: 0.5622
Loss at step 780: 0.4795
Loss at step 790: 0.7014
Loss at step 800: 0.4830
Loss at step 810: 0.5776
Loss at step 820: 0.6509
Loss at step 830: 0.5568
Loss at step 840: 0.4010
Loss at step 850: 0.4974
Loss at step 860: 0.5387
Loss at step 870: 0.5582
Loss at step 880: 0.5453
Loss at step 890: 0.6176
Loss at step 900: 0.5763
Loss at step 910: 0.4497
Loss at step 920: 0.3930
Loss at step 930: 0.7440
Loss at step 940: 0.5648
Loss at step 950: 0.3751
Loss at step 960: 0.4530
Loss at step 970: 0.3602
Loss at step 980: 0.4863
Loss at step 990: 0.4477
Loss at step 1000: 0.6402
Loss at step 1010: 0.4505
Loss at step 1020: 0.6069
Loss at step 1030: 0.3506
Loss at step 1040: 0.3192
Loss at step 1050: 0.3252
Loss at step 1060: 0.4955
Loss at step 1070: 0.2392
Loss at step 1080: 0.6029
Loss at step 1090: 0.3922
Loss at step 1100: 0.4412
Loss at step 1110: 0.2713
Loss at step 1120: 0.5940
Loss at step 1130: 0.3809
Loss at step 1140: 0.2848
Loss at step 1150: 0.3968
Loss at step 1160: 0.4733
Loss at step 1170: 0.4110
Loss at step 1180: 0.3930
Loss at step 1190: 0.3940
Loss at step 1200: 0.3136
Loss at step 1210: 0.3323
Loss at step 1220: 0.4394
Loss at step 1230: 0.5120
Loss at step 1240: 0.5331
Loss at step 1250: 0.2209
Loss at step 1260: 0.5174
Loss at step 1270: 0.3539
Loss at step 1280: 0.3272
Loss at step 1290: 0.2755
Loss at step 1300: 0.2823
Loss at step 1310: 0.3093
Loss at step 1320: 0.5923
Loss at step 1330: 0.2910
Loss at step 1340: 0.3607
Loss at step 1350: 0.3962
Loss at step 1360: 0.2842
Loss at step 1370: 0.2095
Loss at step 1380: 0.4533
Loss at step 1390: 0.6763
Loss at step 1400: 0.5128
Loss at step 1410: 0.3425
Loss at step 1420: 0.2530
Loss at step 1430: 0.2287
Loss at step 1440: 0.5867
Loss at step 1450: 0.4016
Loss at step 1460: 0.3813
Loss at step 1470: 0.2885
Loss at step 1480: 0.3369
Loss at step 1490: 0.2600
Loss at step 1500: 0.1841
Loss at step 1510: 0.2779
Loss at step 1520: 0.3581
Loss at step 1530: 0.2951
Loss at step 1540: 0.2065
Loss at step 1550: 0.1987
Loss at step 1560: 0.3485
Loss at step 1570: 0.2698
Loss at step 1580: 0.3865
Loss at step 1590: 0.3896
Loss at step 1600: 0.2466
Loss at step 1610: 0.3812
Loss at step 1620: 0.3473
Loss at step 1630: 0.4037
Loss at step 1640: 0.1446
Loss at step 1650: 0.1553
Loss at step 1660: 0.3325
Loss at step 1670: 0.2351
Loss at step 1680: 0.2893
Loss at step 1690: 0.3029
Loss at step 1700: 0.3171
Loss at step 1710: 0.2751
Loss at step 1720: 0.6470
Loss at step 1730: 0.1657
Loss at step 1740: 0.3221
Loss at step 1750: 0.2411
Loss at step 1760: 0.0865
Loss at step 1770: 0.4187
Loss at step 1780: 0.2692
Loss at step 1790: 0.1600
Loss at step 1800: 0.2243
Loss at step 1810: 0.0409
Loss at step 1820: 0.3599
Loss at step 1830: 0.0302
Loss at step 1840: 0.3276
Loss at step 1850: 0.0949
Loss at step 1860: 0.2553
Loss at step 1870: 0.2002
Loss at step 1880: 0.3865
Loss at step 1890: 0.2003
Loss at step 1900: 0.3068
Loss at step 1910: 0.4065
Loss at step 1920: 0.2804
Loss at step 1930: 0.1097
Loss at step 1940: 0.1051
Loss at step 1950: 0.3668
Loss at step 1960: 0.1804
Loss at step 1970: 0.1477
Loss at step 1980: 0.1498
Loss at step 1990: 0.1201
Loss at step 2000: 0.1025
Loss at step 2010: 0.1473
Loss at step 2020: 0.1483
Loss at step 2030: 0.1949
Loss at step 2040: 0.2670
Loss at step 2050: 0.2770
Loss at step 2060: 0.2588
Loss at step 2070: 0.2377
Loss at step 2080: 0.1684
Loss at step 2090: 0.3641
Loss at step 2100: 0.2151
Loss at step 2110: 0.1357
Loss at step 2120: 0.2886
Loss at step 2130: 0.1116
Loss at step 2140: 0.1089
Loss at step 2150: 0.1784
Loss at step 2160: 0.1312
Loss at step 2170: 0.2139
Loss at step 2180: 0.2347
Loss at step 2190: 0.3288
Loss at step 2200: 0.2386
Loss at step 2210: 0.0676
Loss at step 2220: 0.1972
Loss at step 2230: 0.1036
Loss at step 2240: 0.0774
Loss at step 2250: 0.2666
Loss at step 2260: 0.0287
Loss at step 2270: 0.1570
Loss at step 2280: 0.1273
Loss at step 2290: 0.2660
Loss at step 2300: 0.0484
Loss at step 2310: 0.1516
Loss at step 2320: 0.2596
Loss at step 2330: 0.1695
Loss at step 2340: 0.1364
Loss at step 2350: 0.1104
Loss at step 2360: 0.3662
Loss at step 2370: 0.3430
Loss at step 2380: 0.1043
Loss at step 2390: 0.2644
Loss at step 2400: 0.1831
Loss at step 2410: 0.1205
Loss at step 2420: 0.1908
Loss at step 2430: 0.0245
Loss at step 2440: 0.2222
Loss at step 2450: 0.0678
Loss at step 2460: 0.0560
Loss at step 2470: 0.1911
Loss at step 2480: 0.1823
Loss at step 2490: 0.0390
Loss at step 2500: 0.2693
Loss at step 2510: 0.0558
Loss at step 2520: 0.0517
Loss at step 2530: 0.2532
Loss at step 2540: 0.0922
Loss at step 2550: 0.2112
Loss at step 2560: 0.1012
Loss at step 2570: 0.2007
Loss at step 2580: 0.1866
Loss at step 2590: 0.0946
Loss at step 2600: 0.0605
Loss at step 2610: 0.0855
Loss at step 2620: 0.3647
Loss at step 2630: 0.1106
Loss at step 2640: 0.0887
Loss at step 2650: 0.0314
Loss at step 2660: 0.2518
Loss at step 2670: 0.1581
Loss at step 2680: 0.1755
Loss at step 2690: 0.0903
Loss at step 2700: 0.4588
Loss at step 2710: 0.2394
Loss at step 2720: 0.1574
Loss at step 2730: 0.1427
Loss at step 2740: 0.4324
Loss at step 2750: 0.0714
Loss at step 2760: 0.0957
Loss at step 2770: 0.2676
Loss at step 2780: 0.2173
Loss at step 2790: 0.0314
Loss at step 2800: 0.1898
Loss at step 2810: 0.4292
Loss at step 2820: 0.0549
Loss at step 2830: 0.0997
Loss at step 2840: 0.0445
Loss at step 2850: 0.2681
Loss at step 2860: 0.0734
Loss at step 2870: 0.1469
Loss at step 2880: 0.0212
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.547215, 'precision': [0.679834, 0.507346, 0.467002], 'recall': [0.524038, 0.653382, 0.428489], 'f1': [0.591855, 0.571177, 0.446917]}
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x755dd0969700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 83320 of the training set: {'input_ids': [101, 1030, 2310, 21885, 5644, 6279, 6442, 9951, 2326, 2525, 25141, 3064, 1996, 2799, 2099, 1998, 2117, 2051, 1996, 4274, 3632, 2041, 102, 8013, 2003, 17949, 2055, 1996, 4274, 2326, 2029, 2003, 2025, 2551, 1012, 4005, 2163, 2008, 2023, 3277, 2003, 2949, 1998, 11186, 2005, 1996, 3861, 1997, 1996, 4070, 1998, 2036, 2163, 2008, 2027, 2031, 1037, 3553, 2298, 2046, 2023, 3277, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 8655
Loss at step 10: 0.9841
Loss at step 20: 1.0290
Loss at step 30: 0.8958
Loss at step 40: 1.1063
Loss at step 50: 0.7665
Loss at step 60: 1.1717
Loss at step 70: 0.7787
Loss at step 80: 0.8785
Loss at step 90: 0.9307
Loss at step 100: 0.9444
Loss at step 110: 0.8200
Loss at step 120: 0.9162
Loss at step 130: 1.0273
Loss at step 140: 0.9849
Loss at step 150: 0.8466
Loss at step 160: 0.9728
Loss at step 170: 0.9802
Loss at step 180: 0.9027
Loss at step 190: 0.8232
Loss at step 200: 1.0056
Loss at step 210: 0.7596
Loss at step 220: 0.9751
Loss at step 230: 0.8198
Loss at step 240: 0.9254
Loss at step 250: 0.9247
Loss at step 260: 0.9077
Loss at step 270: 0.8100
Loss at step 280: 0.8513
Loss at step 290: 0.9319
Loss at step 300: 0.7880
Loss at step 310: 0.7348
Loss at step 320: 0.7221
Loss at step 330: 0.9254
Loss at step 340: 0.6225
Loss at step 350: 0.7834
Loss at step 360: 0.7118
Loss at step 370: 0.6483
Loss at step 380: 0.8001
Loss at step 390: 0.8113
Loss at step 400: 1.0467
Loss at step 410: 0.6732
Loss at step 420: 0.7971
Loss at step 430: 0.6171
Loss at step 440: 0.7005
Loss at step 450: 0.6471
Loss at step 460: 0.7324
Loss at step 470: 0.7721
Loss at step 480: 0.7328
Loss at step 490: 0.7172
Loss at step 500: 0.7507
Loss at step 510: 0.4797
Loss at step 520: 0.5074
Loss at step 530: 0.8072
Loss at step 540: 0.5815
Loss at step 550: 0.6806
Loss at step 560: 0.4999
Loss at step 570: 0.6190
Loss at step 580: 0.8453
Loss at step 590: 0.7843
Loss at step 600: 0.6234
Loss at step 610: 0.6992
Loss at step 620: 0.6448
Loss at step 630: 0.5057
Loss at step 640: 0.7299
Loss at step 650: 0.7094
Loss at step 660: 0.5062
Loss at step 670: 0.6774
Loss at step 680: 0.6283
Loss at step 690: 0.6846
Loss at step 700: 0.5493
Loss at step 710: 0.5691
Loss at step 720: 0.9779
Loss at step 730: 0.6309
Loss at step 740: 0.4806
Loss at step 750: 0.5491
Loss at step 760: 0.5017
Loss at step 770: 0.5622
Loss at step 780: 0.4795
Loss at step 790: 0.7014
Loss at step 800: 0.4830
Loss at step 810: 0.5776
Loss at step 820: 0.6509
Loss at step 830: 0.5568
Loss at step 840: 0.4010
Loss at step 850: 0.4974
Loss at step 860: 0.5387
Loss at step 870: 0.5582
Loss at step 880: 0.5453
Loss at step 890: 0.6176
Loss at step 900: 0.5763
Loss at step 910: 0.4497
Loss at step 920: 0.3930
Loss at step 930: 0.7440
Loss at step 940: 0.5648
Loss at step 950: 0.3751
Loss at step 960: 0.4530
Loss at step 970: 0.3602
Loss at step 980: 0.4863
Loss at step 990: 0.4477
Loss at step 1000: 0.6402
Loss at step 1010: 0.4505
Loss at step 1020: 0.6069
Loss at step 1030: 0.3506
Loss at step 1040: 0.3192
Loss at step 1050: 0.3252
Loss at step 1060: 0.4955
Loss at step 1070: 0.2392
Loss at step 1080: 0.6029
Loss at step 1090: 0.3922
Loss at step 1100: 0.4412
Loss at step 1110: 0.2713
Loss at step 1120: 0.5940
Loss at step 1130: 0.3809
Loss at step 1140: 0.2848
Loss at step 1150: 0.3968
Loss at step 1160: 0.4733
Loss at step 1170: 0.4110
Loss at step 1180: 0.3930
Loss at step 1190: 0.3940
Loss at step 1200: 0.3136
Loss at step 1210: 0.3323
Loss at step 1220: 0.4394
Loss at step 1230: 0.5120
Loss at step 1240: 0.5331
Loss at step 1250: 0.2209
Loss at step 1260: 0.5174
Loss at step 1270: 0.3539
Loss at step 1280: 0.3272
Loss at step 1290: 0.2755
Loss at step 1300: 0.2823
Loss at step 1310: 0.3093
Loss at step 1320: 0.5923
Loss at step 1330: 0.2910
Loss at step 1340: 0.3607
Loss at step 1350: 0.3962
Loss at step 1360: 0.2842
Loss at step 1370: 0.2095
Loss at step 1380: 0.4533
Loss at step 1390: 0.6763
Loss at step 1400: 0.5128
Loss at step 1410: 0.3425
Loss at step 1420: 0.2530
Loss at step 1430: 0.2287
Loss at step 1440: 0.5867
Loss at step 1450: 0.4016
Loss at step 1460: 0.3813
Loss at step 1470: 0.2885
Loss at step 1480: 0.3369
Loss at step 1490: 0.2600
Loss at step 1500: 0.1841
Loss at step 1510: 0.2779
Loss at step 1520: 0.3581
Loss at step 1530: 0.2951
Loss at step 1540: 0.2065
Loss at step 1550: 0.1987
Loss at step 1560: 0.3485
Loss at step 1570: 0.2698
Loss at step 1580: 0.3865
Loss at step 1590: 0.3896
Loss at step 1600: 0.2466
Loss at step 1610: 0.3812
Loss at step 1620: 0.3473
Loss at step 1630: 0.4037
Loss at step 1640: 0.1446
Loss at step 1650: 0.1553
Loss at step 1660: 0.3325
Loss at step 1670: 0.2351
Loss at step 1680: 0.2893
Loss at step 1690: 0.3029
Loss at step 1700: 0.3171
Loss at step 1710: 0.2751
Loss at step 1720: 0.6470
Loss at step 1730: 0.1657
Loss at step 1740: 0.3221
Loss at step 1750: 0.2411
Loss at step 1760: 0.0865
Loss at step 1770: 0.4187
Loss at step 1780: 0.2692
Loss at step 1790: 0.1600
Loss at step 1800: 0.2243
Loss at step 1810: 0.0409
Loss at step 1820: 0.3599
Loss at step 1830: 0.0302
Loss at step 1840: 0.3276
Loss at step 1850: 0.0949
Loss at step 1860: 0.2553
Loss at step 1870: 0.2002
Loss at step 1880: 0.3865
Loss at step 1890: 0.2003
Loss at step 1900: 0.3068
Loss at step 1910: 0.4065
Loss at step 1920: 0.2804
Loss at step 1930: 0.1097
Loss at step 1940: 0.1051
Loss at step 1950: 0.3668
Loss at step 1960: 0.1804
Loss at step 1970: 0.1477
Loss at step 1980: 0.1498
Loss at step 1990: 0.1201
Loss at step 2000: 0.1025
Loss at step 2010: 0.1473
Loss at step 2020: 0.1483
Loss at step 2030: 0.1949
Loss at step 2040: 0.2670
Loss at step 2050: 0.2770
Loss at step 2060: 0.2588
Loss at step 2070: 0.2377
Loss at step 2080: 0.1684
Loss at step 2090: 0.3641
Loss at step 2100: 0.2151
Loss at step 2110: 0.1357
Loss at step 2120: 0.2886
Loss at step 2130: 0.1116
Loss at step 2140: 0.1089
Loss at step 2150: 0.1784
Loss at step 2160: 0.1312
Loss at step 2170: 0.2139
Loss at step 2180: 0.2347
Loss at step 2190: 0.3288
Loss at step 2200: 0.2386
Loss at step 2210: 0.0676
Loss at step 2220: 0.1972
Loss at step 2230: 0.1036
Loss at step 2240: 0.0774
Loss at step 2250: 0.2666
Loss at step 2260: 0.0287
Loss at step 2270: 0.1570
Loss at step 2280: 0.1273
Loss at step 2290: 0.2660
Loss at step 2300: 0.0484
Loss at step 2310: 0.1516
Loss at step 2320: 0.2596
Loss at step 2330: 0.1695
Loss at step 2340: 0.1364
Loss at step 2350: 0.1104
Loss at step 2360: 0.3662
Loss at step 2370: 0.3430
Loss at step 2380: 0.1043
Loss at step 2390: 0.2644
Loss at step 2400: 0.1831
Loss at step 2410: 0.1205
Loss at step 2420: 0.1908
Loss at step 2430: 0.0245
Loss at step 2440: 0.2222
Loss at step 2450: 0.0678
Loss at step 2460: 0.0560
Loss at step 2470: 0.1911
Loss at step 2480: 0.1823
Loss at step 2490: 0.0390
Loss at step 2500: 0.2693
Loss at step 2510: 0.0558
Loss at step 2520: 0.0517
Loss at step 2530: 0.2532
Loss at step 2540: 0.0922
Loss at step 2550: 0.2112
Loss at step 2560: 0.1012
Loss at step 2570: 0.2007
Loss at step 2580: 0.1866
Loss at step 2590: 0.0946
Loss at step 2600: 0.0605
Loss at step 2610: 0.0855
Loss at step 2620: 0.3647
Loss at step 2630: 0.1106
Loss at step 2640: 0.0887
Loss at step 2650: 0.0314
Loss at step 2660: 0.2518
Loss at step 2670: 0.1581
Loss at step 2680: 0.1755
Loss at step 2690: 0.0903
Loss at step 2700: 0.4588
Loss at step 2710: 0.2394
Loss at step 2720: 0.1574
Loss at step 2730: 0.1427
Loss at step 2740: 0.4324
Loss at step 2750: 0.0714
Loss at step 2760: 0.0957
Loss at step 2770: 0.2676
Loss at step 2780: 0.2173
Loss at step 2790: 0.0314
Loss at step 2800: 0.1898
Loss at step 2810: 0.4292
Loss at step 2820: 0.0549
Loss at step 2830: 0.0997
Loss at step 2840: 0.0445
Loss at step 2850: 0.2681
Loss at step 2860: 0.0734
Loss at step 2870: 0.1469
Loss at step 2880: 0.0212
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.547215, 'precision': [0.679834, 0.507346, 0.467002], 'recall': [0.524038, 0.653382, 0.428489], 'f1': [0.591855, 0.571177, 0.446917]}
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x71fe935ea700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 83320 of the training set: {'input_ids': [101, 1030, 2310, 21885, 5644, 6279, 6442, 9951, 2326, 2525, 25141, 3064, 1996, 2799, 2099, 1998, 2117, 2051, 1996, 4274, 3632, 2041, 102, 8013, 2003, 17949, 2055, 1996, 4274, 2326, 2029, 2003, 2025, 2551, 1012, 4005, 2163, 2008, 2023, 3277, 2003, 2949, 1998, 11186, 2005, 1996, 3861, 1997, 1996, 4070, 1998, 2036, 2163, 2008, 2027, 2031, 1037, 3553, 2298, 2046, 2023, 3277, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 8655
Loss at step 10: 0.9841
Loss at step 20: 1.0290
Loss at step 30: 0.8958
Loss at step 40: 1.1063
Loss at step 50: 0.7665
Loss at step 60: 1.1717
Loss at step 70: 0.7787
Loss at step 80: 0.8785
Loss at step 90: 0.9307
Loss at step 100: 0.9444
Loss at step 110: 0.8200
Loss at step 120: 0.9162
Loss at step 130: 1.0273
Loss at step 140: 0.9849
Loss at step 150: 0.8466
Loss at step 160: 0.9728
Loss at step 170: 0.9802
Loss at step 180: 0.9027
Loss at step 190: 0.8232
Loss at step 200: 1.0056
Loss at step 210: 0.7596
Loss at step 220: 0.9751
Loss at step 230: 0.8198
Loss at step 240: 0.9254
Loss at step 250: 0.9247
Loss at step 260: 0.9077
Loss at step 270: 0.8100
Loss at step 280: 0.8513
Loss at step 290: 0.9319
Loss at step 300: 0.7880
Loss at step 310: 0.7348
Loss at step 320: 0.7221
Loss at step 330: 0.9254
Loss at step 340: 0.6225
Loss at step 350: 0.7834
Loss at step 360: 0.7118
Loss at step 370: 0.6483
Loss at step 380: 0.8001
Loss at step 390: 0.8113
Loss at step 400: 1.0467
Loss at step 410: 0.6732
Loss at step 420: 0.7971
Loss at step 430: 0.6171
Loss at step 440: 0.7005
Loss at step 450: 0.6471
Loss at step 460: 0.7324
Loss at step 470: 0.7721
Loss at step 480: 0.7328
Loss at step 490: 0.7172
Loss at step 500: 0.7507
Loss at step 510: 0.4797
Loss at step 520: 0.5074
Loss at step 530: 0.8072
Loss at step 540: 0.5815
Loss at step 550: 0.6806
Loss at step 560: 0.4999
Loss at step 570: 0.6190
Loss at step 580: 0.8453
Loss at step 590: 0.7843
Loss at step 600: 0.6234
Loss at step 610: 0.6992
Loss at step 620: 0.6448
Loss at step 630: 0.5057
Loss at step 640: 0.7299
Loss at step 650: 0.7094
Loss at step 660: 0.5062
Loss at step 670: 0.6774
Loss at step 680: 0.6283
Loss at step 690: 0.6846
Loss at step 700: 0.5493
Loss at step 710: 0.5691
Loss at step 720: 0.9779
Loss at step 730: 0.6309
Loss at step 740: 0.4806
Loss at step 750: 0.5491
Loss at step 760: 0.5017
Loss at step 770: 0.5622
Loss at step 780: 0.4795
Loss at step 790: 0.7014
Loss at step 800: 0.4830
Loss at step 810: 0.5776
Loss at step 820: 0.6509
Loss at step 830: 0.5568
Loss at step 840: 0.4010
Loss at step 850: 0.4974
Loss at step 860: 0.5387
Loss at step 870: 0.5582
Loss at step 880: 0.5453
Loss at step 890: 0.6176
Loss at step 900: 0.5763
Loss at step 910: 0.4497
Loss at step 920: 0.3930
Loss at step 930: 0.7440
Loss at step 940: 0.5648
Loss at step 950: 0.3751
Loss at step 960: 0.4530
Loss at step 970: 0.3602
Loss at step 980: 0.4863
Loss at step 990: 0.4477
Loss at step 1000: 0.6402
Loss at step 1010: 0.4505
Loss at step 1020: 0.6069
Loss at step 1030: 0.3506
Loss at step 1040: 0.3192
Loss at step 1050: 0.3252
Loss at step 1060: 0.4955
Loss at step 1070: 0.2392
Loss at step 1080: 0.6029
Loss at step 1090: 0.3922
Loss at step 1100: 0.4412
Loss at step 1110: 0.2713
Loss at step 1120: 0.5940
Loss at step 1130: 0.3809
Loss at step 1140: 0.2848
Loss at step 1150: 0.3968
Loss at step 1160: 0.4733
Loss at step 1170: 0.4110
Loss at step 1180: 0.3930
Loss at step 1190: 0.3940
Loss at step 1200: 0.3136
Loss at step 1210: 0.3323
Loss at step 1220: 0.4394
Loss at step 1230: 0.5120
Loss at step 1240: 0.5331
Loss at step 1250: 0.2209
Loss at step 1260: 0.5174
Loss at step 1270: 0.3539
Loss at step 1280: 0.3272
Loss at step 1290: 0.2755
Loss at step 1300: 0.2823
Loss at step 1310: 0.3093
Loss at step 1320: 0.5923
Loss at step 1330: 0.2910
Loss at step 1340: 0.3607
Loss at step 1350: 0.3962
Loss at step 1360: 0.2842
Loss at step 1370: 0.2095
Loss at step 1380: 0.4533
Loss at step 1390: 0.6763
Loss at step 1400: 0.5128
Loss at step 1410: 0.3425
Loss at step 1420: 0.2530
Loss at step 1430: 0.2287
Loss at step 1440: 0.5867
Loss at step 1450: 0.4016
Loss at step 1460: 0.3813
Loss at step 1470: 0.2885
Loss at step 1480: 0.3369
Loss at step 1490: 0.2600
Loss at step 1500: 0.1841
Loss at step 1510: 0.2779
Loss at step 1520: 0.3581
Loss at step 1530: 0.2951
Loss at step 1540: 0.2065
Loss at step 1550: 0.1987
Loss at step 1560: 0.3485
Loss at step 1570: 0.2698
Loss at step 1580: 0.3865
Loss at step 1590: 0.3896
Loss at step 1600: 0.2466
Loss at step 1610: 0.3812
Loss at step 1620: 0.3473
Loss at step 1630: 0.4037
Loss at step 1640: 0.1446
Loss at step 1650: 0.1553
Loss at step 1660: 0.3325
Loss at step 1670: 0.2351
Loss at step 1680: 0.2893
Loss at step 1690: 0.3029
Loss at step 1700: 0.3171
Loss at step 1710: 0.2751
Loss at step 1720: 0.6470
Loss at step 1730: 0.1657
Loss at step 1740: 0.3221
Loss at step 1750: 0.2411
Loss at step 1760: 0.0865
Loss at step 1770: 0.4187
Loss at step 1780: 0.2692
Loss at step 1790: 0.1600
Loss at step 1800: 0.2243
Loss at step 1810: 0.0409
Loss at step 1820: 0.3599
Loss at step 1830: 0.0302
Loss at step 1840: 0.3276
Loss at step 1850: 0.0949
Loss at step 1860: 0.2553
Loss at step 1870: 0.2002
Loss at step 1880: 0.3865
Loss at step 1890: 0.2003
Loss at step 1900: 0.3068
Loss at step 1910: 0.4065
Loss at step 1920: 0.2804
Loss at step 1930: 0.1097
Loss at step 1940: 0.1051
Loss at step 1950: 0.3668
Loss at step 1960: 0.1804
Loss at step 1970: 0.1477
Loss at step 1980: 0.1498
Loss at step 1990: 0.1201
Loss at step 2000: 0.1025
Loss at step 2010: 0.1473
Loss at step 2020: 0.1483
Loss at step 2030: 0.1949
Loss at step 2040: 0.2670
Loss at step 2050: 0.2770
Loss at step 2060: 0.2588
Loss at step 2070: 0.2377
Loss at step 2080: 0.1684
Loss at step 2090: 0.3641
Loss at step 2100: 0.2151
Loss at step 2110: 0.1357
Loss at step 2120: 0.2886
Loss at step 2130: 0.1116
Loss at step 2140: 0.1089
Loss at step 2150: 0.1784
Loss at step 2160: 0.1312
Loss at step 2170: 0.2139
Loss at step 2180: 0.2347
Loss at step 2190: 0.3288
Loss at step 2200: 0.2386
Loss at step 2210: 0.0676
Loss at step 2220: 0.1972
Loss at step 2230: 0.1036
Loss at step 2240: 0.0774
Loss at step 2250: 0.2666
Loss at step 2260: 0.0287
Loss at step 2270: 0.1570
Loss at step 2280: 0.1273
Loss at step 2290: 0.2660
Loss at step 2300: 0.0484
Loss at step 2310: 0.1516
Loss at step 2320: 0.2596
Loss at step 2330: 0.1695
Loss at step 2340: 0.1364
Loss at step 2350: 0.1104
Loss at step 2360: 0.3662
Loss at step 2370: 0.3430
Loss at step 2380: 0.1043
Loss at step 2390: 0.2644
Loss at step 2400: 0.1831
Loss at step 2410: 0.1205
Loss at step 2420: 0.1908
Loss at step 2430: 0.0245
Loss at step 2440: 0.2222
Loss at step 2450: 0.0678
Loss at step 2460: 0.0560
Loss at step 2470: 0.1911
Loss at step 2480: 0.1823
Loss at step 2490: 0.0390
Loss at step 2500: 0.2693
Loss at step 2510: 0.0558
Loss at step 2520: 0.0517
Loss at step 2530: 0.2532
Loss at step 2540: 0.0922
Loss at step 2550: 0.2112
Loss at step 2560: 0.1012
Loss at step 2570: 0.2007
Loss at step 2580: 0.1866
Loss at step 2590: 0.0946
Loss at step 2600: 0.0605
Loss at step 2610: 0.0855
Loss at step 2620: 0.3647
Loss at step 2630: 0.1106
Loss at step 2640: 0.0887
Loss at step 2650: 0.0314
Loss at step 2660: 0.2518
Loss at step 2670: 0.1581
Loss at step 2680: 0.1755
Loss at step 2690: 0.0903
Loss at step 2700: 0.4588
Loss at step 2710: 0.2394
Loss at step 2720: 0.1574
Loss at step 2730: 0.1427
Loss at step 2740: 0.4324
Loss at step 2750: 0.0714
Loss at step 2760: 0.0957
Loss at step 2770: 0.2676
Loss at step 2780: 0.2173
Loss at step 2790: 0.0314
Loss at step 2800: 0.1898
Loss at step 2810: 0.4292
Loss at step 2820: 0.0549
Loss at step 2830: 0.0997
Loss at step 2840: 0.0445
Loss at step 2850: 0.2681
Loss at step 2860: 0.0734
Loss at step 2870: 0.1469
Loss at step 2880: 0.0212
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.547215, 'precision': [0.679834, 0.507346, 0.467002], 'recall': [0.524038, 0.653382, 0.428489], 'f1': [0.591855, 0.571177, 0.446917]}
