Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x788db886a700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 4092 of the training set: {'input_ids': [101, 8013, 2003, 17949, 2055, 1996, 2326, 2073, 2027, 2024, 4039, 2000, 3962, 8757, 1998, 2036, 10821, 2008, 2023, 2003, 2025, 2551, 1012, 4005, 25380, 2005, 1996, 4751, 1998, 2356, 2000, 1040, 2213, 2440, 2171, 1998, 3024, 24471, 2140, 1012, 102, 101, 1045, 10587, 2113, 2339, 28517, 17195, 11597, 2003, 1996, 2069, 21144, 2094, 2299, 1045, 2064, 2424, 2006, 1030, 10630, 2620, 2620, 2620, 102, 101, 1030, 4700, 17465, 16932, 4931, 22093, 999, 2064, 2017, 2425, 2149, 2054, 2406, 2115, 4070, 2003, 2275, 2000, 1029, 2057, 1005, 2222, 4638, 2477, 2041, 1013, 10381, 102, 101, 1030, 3962, 8757, 16302, 2015, 2142, 2163, 999, 102, 101, 1030, 4700, 17465, 16932, 3093, 4625, 2057, 1005, 2222, 2022, 2583, 2000, 2031, 2035, 1997, 2037, 2774, 2574, 1010, 2021, 2045, 1005, 1055, 18558, 2055, 3962, 8757, 4180, 2182, 1024, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1014, 2072, 2620, 21600, 5714, 14066, 1013, 10381, 102, 101, 1030, 3962, 8757, 16302, 2015, 25430, 4402, 15558, 999, 102, 101, 1030, 4700, 17465, 16932, 4658, 13435, 999, 2017, 2113, 2073, 2000, 2424, 2149, 2065, 2045, 1005, 1055, 2505, 2842, 2017, 2342, 2393, 2007, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1057, 19025, 19666, 2595, 2497, 2480, 2078, 100, 1013, 10381, 102, 101, 1030, 3962, 8757, 16302, 2015, 4067, 2017, 999, 100, 102, 101, 1030, 4700, 17465, 16932, 2017, 1005, 2128, 2467, 6160, 999, 100, 1013, 10381, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2, 2, 0, 2, 0, 1, 0, 0]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 6594
Loss at step 10: 1.1083
Loss at step 20: 1.0932
Loss at step 30: 1.1125
Loss at step 40: 1.0735
Loss at step 50: 1.0145
Loss at step 60: 1.0981
Loss at step 70: 1.0678
Loss at step 80: 1.0661
Loss at step 90: 1.1532
Loss at step 100: 0.9939
Loss at step 110: 1.0614
Loss at step 120: 1.0334
Loss at step 130: 1.0475
Loss at step 140: 1.1298
Loss at step 150: 1.0862
Loss at step 160: 1.0562
Loss at step 170: 1.0590
Loss at step 180: 1.0542
Loss at step 190: 1.1185
Loss at step 200: 1.0759
Loss at step 210: 1.0588
Loss at step 220: 1.0274
Loss at step 230: 1.1433
Loss at step 240: 0.9840
Loss at step 250: 0.9933
Loss at step 260: 1.0575
Loss at step 270: 1.0707
Loss at step 280: 1.0377
Loss at step 290: 1.0394
Loss at step 300: 1.0529
Loss at step 310: 0.9648
Loss at step 320: 1.1274
Loss at step 330: 1.0659
Loss at step 340: 0.9705
Loss at step 350: 1.0992
Loss at step 360: 1.0319
Loss at step 370: 1.1231
Loss at step 380: 1.0133
Loss at step 390: 1.0610
Loss at step 400: 0.9995
Loss at step 410: 1.1584
Loss at step 420: 1.0841
Loss at step 430: 1.0413
Loss at step 440: 0.9647
Loss at step 450: 1.0143
Loss at step 460: 1.1003
Loss at step 470: 1.0699
Loss at step 480: 0.9553
Loss at step 490: 1.0704
Loss at step 500: 1.1905
Loss at step 510: 0.9205
Loss at step 520: 1.0400
Loss at step 530: 1.0591
Loss at step 540: 1.0173
Loss at step 550: 1.0640
Loss at step 560: 0.9399
Loss at step 570: 0.9966
Loss at step 580: 0.9978
Loss at step 590: 1.0387
Loss at step 600: 0.9914
Loss at step 610: 1.1303
Loss at step 620: 1.0338
Loss at step 630: 1.0259
Loss at step 640: 1.0508
Loss at step 650: 1.0372
Loss at step 660: 0.9775
Loss at step 670: 0.8978
Loss at step 680: 0.9257
Loss at step 690: 1.1804
Loss at step 700: 1.0458
Loss at step 710: 1.0440
Loss at step 720: 1.0017
Loss at step 730: 0.9829
Loss at step 740: 0.9201
Loss at step 750: 0.9290
Loss at step 760: 0.8725
Loss at step 770: 0.9571
Loss at step 780: 1.1073
Loss at step 790: 0.9619
Loss at step 800: 1.0013
Loss at step 810: 0.9677
Loss at step 820: 0.9339
Loss at step 830: 0.9173
Loss at step 840: 0.9793
Loss at step 850: 0.9324
Loss at step 860: 0.8693
Loss at step 870: 0.9570
Loss at step 880: 1.0549
Loss at step 890: 1.0584
Loss at step 900: 0.9743
Loss at step 910: 1.0950
Loss at step 920: 0.9906
Loss at step 930: 0.8720
Loss at step 940: 1.0735
Loss at step 950: 1.0296
Loss at step 960: 0.8163
Loss at step 970: 1.2438
Loss at step 980: 0.8362
Loss at step 990: 0.9211
Loss at step 1000: 0.9181
Loss at step 1010: 0.9432
Loss at step 1020: 0.8418
Loss at step 1030: 0.8571
Loss at step 1040: 1.0083
Loss at step 1050: 0.7625
Loss at step 1060: 1.0713
Loss at step 1070: 0.7419
Loss at step 1080: 1.0078
Loss at step 1090: 0.8996
Loss at step 1100: 0.7956
Loss at step 1110: 0.8818
Loss at step 1120: 0.8238
Loss at step 1130: 0.8786
Loss at step 1140: 0.9024
Loss at step 1150: 0.8589
Loss at step 1160: 0.9435
Loss at step 1170: 0.9862
Loss at step 1180: 0.6844
Loss at step 1190: 1.0744
Loss at step 1200: 0.9123
Loss at step 1210: 0.8358
Loss at step 1220: 0.8971
Loss at step 1230: 0.8110
Loss at step 1240: 0.7007
Loss at step 1250: 0.9284
Loss at step 1260: 0.7412
Loss at step 1270: 0.9322
Loss at step 1280: 0.7578
Loss at step 1290: 1.0022
Loss at step 1300: 0.9541
Loss at step 1310: 0.7462
Loss at step 1320: 1.0453
Loss at step 1330: 0.6851
Loss at step 1340: 0.7567
Loss at step 1350: 0.7437
Loss at step 1360: 0.7668
Loss at step 1370: 0.8359
Loss at step 1380: 0.6655
Loss at step 1390: 0.9630
Loss at step 1400: 0.8909
Loss at step 1410: 0.9391
Loss at step 1420: 0.7181
Loss at step 1430: 0.6789
Loss at step 1440: 0.7774
Loss at step 1450: 0.7910
Loss at step 1460: 0.7458
Loss at step 1470: 0.7240
Loss at step 1480: 0.7510
Loss at step 1490: 1.0467
Loss at step 1500: 0.9841
Loss at step 1510: 0.7913
Loss at step 1520: 0.7394
Loss at step 1530: 0.6938
Loss at step 1540: 0.8084
Loss at step 1550: 0.6220
Loss at step 1560: 0.7256
Loss at step 1570: 0.9711
Loss at step 1580: 0.5448
Loss at step 1590: 0.6355
Loss at step 1600: 0.7500
Loss at step 1610: 0.6526
Loss at step 1620: 0.8227
Loss at step 1630: 0.7060
Loss at step 1640: 0.5534
Loss at step 1650: 0.7655
Loss at step 1660: 0.6628
Loss at step 1670: 0.5762
Loss at step 1680: 0.7017
Loss at step 1690: 0.5595
Loss at step 1700: 0.5160
Loss at step 1710: 0.6817
Loss at step 1720: 0.5461
Loss at step 1730: 0.7410
Loss at step 1740: 0.9177
Loss at step 1750: 0.5507
Loss at step 1760: 0.6725
Loss at step 1770: 0.4740
Loss at step 1780: 0.5651
Loss at step 1790: 0.5486
Loss at step 1800: 0.3104
Loss at step 1810: 0.8387
Loss at step 1820: 0.4399
Loss at step 1830: 0.7888
Loss at step 1840: 0.4063
Loss at step 1850: 0.6136
Loss at step 1860: 0.6764
Loss at step 1870: 0.3852
Loss at step 1880: 0.6967
Loss at step 1890: 0.5084
Loss at step 1900: 0.3286
Loss at step 1910: 0.5102
Loss at step 1920: 0.4812
Loss at step 1930: 0.4079
Loss at step 1940: 0.6466
Loss at step 1950: 0.4587
Loss at step 1960: 0.5683
Loss at step 1970: 0.3694
Loss at step 1980: 0.3797
Loss at step 1990: 0.5265
Loss at step 2000: 0.1617
Loss at step 2010: 0.7963
Loss at step 2020: 0.7657
Loss at step 2030: 0.4585
Loss at step 2040: 0.2360
Loss at step 2050: 0.3460
Loss at step 2060: 0.5251
Loss at step 2070: 0.4981
Loss at step 2080: 0.9458
Loss at step 2090: 0.4517
Loss at step 2100: 0.6344
Loss at step 2110: 0.4474
Loss at step 2120: 0.2322
Loss at step 2130: 0.3179
Loss at step 2140: 0.6871
Loss at step 2150: 0.1407
Loss at step 2160: 0.3740
Loss at step 2170: 0.4330
Loss at step 2180: 0.3549
Loss at step 2190: 0.3970
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.541182, 'precision': [0.672368, 0.485664, 0.499247], 'recall': [0.515574, 0.676171, 0.385017], 'f1': [0.583623, 0.565299, 0.434754]}
